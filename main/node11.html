
<H3><A ID="SECTION00022300000000000000">
Fungsi Aktivasi</A>
</H3>

<P>
Fungsi aktivasi digunakan untuk mengubah level aktivasi pada suatu neuron menjadi sebuah sinyal output (#KarlicOlgacPerformanceAnalysis#<tex2html_cite_par_mark>##). Pada TA ini, terdapat 2 fungsi aktivasi yang digunakan. Pada hidden layer, digunakan fungsi aktivasi <SPAN  CLASS="textit">Rectified Linear Unit (RELU)</SPAN>. Fungsi aktivasi <SPAN  CLASS="textit">RELU</SPAN> didefinisikan dengan persamaan <A HREF=<tex2html_cr_mark>#eq:aktivasi_relu#537><tex2html_cr_mark></A> (#glorot2011deep#<tex2html_cite_par_mark>##).

<P>
<P></P>
<DIV CLASS="mathdisplay"><A ID="eq:aktivasi_relu"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math626#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay3691#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">3</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
RELU menjadi pilihan karna memiliki performa konvergensi yang lebih baik dibanding sigmoid (#Krizhevsky:2012:ICD:2999134.2999257#<tex2html_cite_par_mark>##). Untuk selanjutnya, pada neuron <SPAN  CLASS="textit">output</SPAN>, digunakan fungsi aktivasi linear. Fungsi aktivasi linear didefinisikan dengan persamaan <A HREF=<tex2html_cr_mark>#eq:aktivasi_linear#544><tex2html_cr_mark></A> (#MLBishop#<tex2html_cite_par_mark>##).

<P>
